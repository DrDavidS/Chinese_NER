{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# BERT imports\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences  # padding句子用\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  主要信息记录\n",
    "\n",
    "- 日期：3月5日\n",
    "- 本地CV：0.7184718\n",
    "- 线上CV：0.69275504\n",
    "\n",
    "- 参数：\n",
    "    - LR：2e-5\n",
    "    - epoch:3\n",
    "\n",
    "- 思路：BERT梭哈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本： 1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch 版本： {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available:  True\n",
      "GPU numbers:  2\n",
      "device_name:  Tesla M40 24GB\n",
      "当前设备编号：1\n"
     ]
    }
   ],
   "source": [
    "# GPUcheck\n",
    "\n",
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU numbers: \", n_gpu)\n",
    "    print(\"device_name: \", torch.cuda.get_device_name(0))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:1\")  # 注意选择\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"当前设备编号：{torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled = pd.read_csv('./input/train_dataset/nCoV_100k_train_labled_utf8.csv', encoding='utf-8')\n",
    "\n",
    "train_labeled.rename(columns = {\"微博id\": \"Weibo_ID\",\n",
    "                                \"微博发布时间\": \"Publish_Time\", \n",
    "                                \"发布人账号\": \"Account_ID\",\n",
    "                                \"微博中文内容\": \"Chinese_Content\",\n",
    "                                \"微博图片\": \"Pictures\",\n",
    "                                \"微博视频\": \"Videos\",\n",
    "                                \"情感倾向\": \"Labels\"},  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weibo_ID</th>\n",
       "      <th>Publish_Time</th>\n",
       "      <th>Account_ID</th>\n",
       "      <th>Chinese_Content</th>\n",
       "      <th>Pictures</th>\n",
       "      <th>Videos</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4456072029125500</td>\n",
       "      <td>01月01日 23:50</td>\n",
       "      <td>存曦1988</td>\n",
       "      <td>写在年末冬初孩子流感的第五天，我们仍然没有忘记热情拥抱这2020年的第一天。带着一丝迷信，早...</td>\n",
       "      <td>['https://ww2.sinaimg.cn/orj360/005VnA1zly1gah...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4456074167480980</td>\n",
       "      <td>01月01日 23:58</td>\n",
       "      <td>LunaKrys</td>\n",
       "      <td>开年大模型…累到以为自己发烧了腰疼膝盖疼腿疼胳膊疼脖子疼#Luna的Krystallife#?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4456054253264520</td>\n",
       "      <td>01月01日 22:39</td>\n",
       "      <td>小王爷学辩论o_O</td>\n",
       "      <td>邱晨这就是我爹，爹，发烧快好，毕竟美好的假期拿来养病不太好，假期还是要好好享受快乐，爹，新...</td>\n",
       "      <td>['https://ww2.sinaimg.cn/thumb150/006ymYXKgy1g...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Weibo_ID  Publish_Time Account_ID  \\\n",
       "0  4456072029125500  01月01日 23:50     存曦1988   \n",
       "1  4456074167480980  01月01日 23:58   LunaKrys   \n",
       "2  4456054253264520  01月01日 22:39  小王爷学辩论o_O   \n",
       "\n",
       "                                     Chinese_Content  \\\n",
       "0  写在年末冬初孩子流感的第五天，我们仍然没有忘记热情拥抱这2020年的第一天。带着一丝迷信，早...   \n",
       "1    开年大模型…累到以为自己发烧了腰疼膝盖疼腿疼胳膊疼脖子疼#Luna的Krystallife#?   \n",
       "2  邱晨这就是我爹，爹，发烧快好，毕竟美好的假期拿来养病不太好，假期还是要好好享受快乐，爹，新...   \n",
       "\n",
       "                                            Pictures Videos Labels  \n",
       "0  ['https://ww2.sinaimg.cn/orj360/005VnA1zly1gah...     []      0  \n",
       "1                                                 []     []     -1  \n",
       "2  ['https://ww2.sinaimg.cn/thumb150/006ymYXKgy1g...     []      1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前几句\n",
    "train_labeled.iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清洗数据\n",
    "train_labeled_clean = train_labeled[(train_labeled['Labels'] == '-1') |\n",
    "                                    (train_labeled['Labels'] == '0') |\n",
    "                                    (train_labeled['Labels'] == '1') ]\n",
    "\n",
    "train_labeled_clean = train_labeled_clean.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [content for content in train_labeled_clean['Chinese_Content'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'写在年末冬初孩子流感的第五天，我们仍然没有忘记热情拥抱这2020年的第一天。带着一丝迷信，早晨给孩子穿上红色的羽绒服羽绒裤，祈祷新的一年，孩子们身体康健。仍然会有一丝焦虑，焦虑我的孩子为什么会过早的懂事，从两岁多开始关注我的情绪，会深沉地说：妈妈，你终于笑了！这句话像刀子一样扎入我?展开全文c'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for label in train_labeled_clean['Labels'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99560\n"
     ]
    }
   ],
   "source": [
    "assert len(sentences) == len(labels)\n",
    "\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['写在年末冬初孩子流感的第五天，我们仍然没有忘记热情拥抱这2020年的第一天。带着一丝迷信，早晨给孩子穿上红色的羽绒服羽绒裤，祈祷新的一年，孩子们身体康健。仍然会有一丝焦虑，焦虑我的孩子为什么会过早的懂事，从两岁多开始关注我的情绪，会深沉地说：妈妈，你终于笑了！这句话像刀子一样扎入我?展开全文c', '开年大模型…累到以为自己发烧了腰疼膝盖疼腿疼胳膊疼脖子疼#Luna的Krystallife#?', '\\ue627邱晨这就是我爹，爹，发烧快好，毕竟美好的假期拿来养病不太好，假期还是要好好享受快乐，爹，新年快乐，发烧好了就去浪吧，快快乐乐的度过这个美好假期，说不定以后就没有了嗷@邱晨虫仔2泉州·泉州理工学院?']\n",
      "['0', '-1', '1']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0:3])\n",
    "print(labels[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_bert.BertTokenizer at 0x7fa87c324610>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('../BERT-NER/bert-chinese/', do_lower_case=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 前的第一句话：\n",
      "写在年末冬初孩子流感的第五天，我们仍然没有忘记热情拥抱这2020年的第一天。带着一丝迷信，早晨给孩子穿上红色的羽绒服羽绒裤，祈祷新的一年，孩子们身体康健。仍然会有一丝焦虑，焦虑我的孩子为什么会过早的懂事，从两岁多开始关注我的情绪，会深沉地说：妈妈，你终于笑了！这句话像刀子一样扎入我?展开全文c\n",
      "\n",
      "Tokenize 后的第一句话: \n",
      "[101, 1091, 1762, 2399, 3314, 1100, 1159, 2111, 2094, 3837, 2697, 4638, 5018, 758, 1921, 8024, 2769, 812, 793, 4197, 3766, 3300, 2563, 6381, 4178, 2658, 2881, 2849, 6821, 8439, 2399, 4638, 5018, 671, 1921, 511, 2372, 4708, 671, 692, 6837, 928, 8024, 3193, 3247, 5314, 2111, 2094, 4959, 677, 5273, 5682, 4638, 5417, 5309, 3302, 5417, 5309, 6175, 8024, 4857, 4876, 3173, 4638, 671, 2399, 8024, 2111, 2094, 812, 6716, 860, 2434, 978, 511, 793, 4197, 833, 3300, 671, 692, 4193, 5991, 8024, 4193, 5991, 2769, 4638, 2111, 2094, 711, 784, 720, 833, 6814, 3193, 4638, 2743, 752, 8024, 794, 697, 2259, 1914, 2458, 1993, 1068, 3800, 2769, 4638, 2658, 5328, 8024, 833, 3918, 3756, 1765, 6432, 8038, 1968, 1968, 8024, 872, 5303, 754, 5010, 749, 8013, 6821, 1368, 6413, 1008, 1143, 2094, 671, 3416, 2799, 1057, 2769, 136, 2245, 2458, 1059, 3152, 145, 102]\n"
     ]
    }
   ],
   "source": [
    "# 这句话的input_ids\n",
    "print(f\"Tokenize 前的第一句话：\\n{sentences[0]}\\n\")\n",
    "print(f\"Tokenize 后的第一句话: \\n{tokenized_texts[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99560\n"
     ]
    }
   ],
   "source": [
    "print (len(tokenized_texts))  # 99560句话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子最长长度\n",
    "MAX_LEN = 300\n",
    "\n",
    "# 输入padding\n",
    "# 此函数在keras里面\n",
    "input_ids = pad_sequences([txt for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", \n",
    "                          truncating=\"post\", \n",
    "                          padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 前的第一句话：\n",
      "\n",
      "写在年末冬初孩子流感的第五天，我们仍然没有忘记热情拥抱这2020年的第一天。带着一丝迷信，早晨给孩子穿上红色的羽绒服羽绒裤，祈祷新的一年，孩子们身体康健。仍然会有一丝焦虑，焦虑我的孩子为什么会过早的懂事，从两岁多开始关注我的情绪，会深沉地说：妈妈，你终于笑了！这句话像刀子一样扎入我?展开全文c\n",
      "\n",
      "\n",
      "Tokenize 后的第一句话: \n",
      "\n",
      "[101, 1091, 1762, 2399, 3314, 1100, 1159, 2111, 2094, 3837, 2697, 4638, 5018, 758, 1921, 8024, 2769, 812, 793, 4197, 3766, 3300, 2563, 6381, 4178, 2658, 2881, 2849, 6821, 8439, 2399, 4638, 5018, 671, 1921, 511, 2372, 4708, 671, 692, 6837, 928, 8024, 3193, 3247, 5314, 2111, 2094, 4959, 677, 5273, 5682, 4638, 5417, 5309, 3302, 5417, 5309, 6175, 8024, 4857, 4876, 3173, 4638, 671, 2399, 8024, 2111, 2094, 812, 6716, 860, 2434, 978, 511, 793, 4197, 833, 3300, 671, 692, 4193, 5991, 8024, 4193, 5991, 2769, 4638, 2111, 2094, 711, 784, 720, 833, 6814, 3193, 4638, 2743, 752, 8024, 794, 697, 2259, 1914, 2458, 1993, 1068, 3800, 2769, 4638, 2658, 5328, 8024, 833, 3918, 3756, 1765, 6432, 8038, 1968, 1968, 8024, 872, 5303, 754, 5010, 749, 8013, 6821, 1368, 6413, 1008, 1143, 2094, 671, 3416, 2799, 1057, 2769, 136, 2245, 2458, 1059, 3152, 145, 102]\n",
      "\n",
      "\n",
      "Padding 后的第一句话： \n",
      "\n",
      "[ 101 1091 1762 2399 3314 1100 1159 2111 2094 3837 2697 4638 5018  758\n",
      " 1921 8024 2769  812  793 4197 3766 3300 2563 6381 4178 2658 2881 2849\n",
      " 6821 8439 2399 4638 5018  671 1921  511 2372 4708  671  692 6837  928\n",
      " 8024 3193 3247 5314 2111 2094 4959  677 5273 5682 4638 5417 5309 3302\n",
      " 5417 5309 6175 8024 4857 4876 3173 4638  671 2399 8024 2111 2094  812\n",
      " 6716  860 2434  978  511  793 4197  833 3300  671  692 4193 5991 8024\n",
      " 4193 5991 2769 4638 2111 2094  711  784  720  833 6814 3193 4638 2743\n",
      "  752 8024  794  697 2259 1914 2458 1993 1068 3800 2769 4638 2658 5328\n",
      " 8024  833 3918 3756 1765 6432 8038 1968 1968 8024  872 5303  754 5010\n",
      "  749 8013 6821 1368 6413 1008 1143 2094  671 3416 2799 1057 2769  136\n",
      " 2245 2458 1059 3152  145  102    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenize 前的第一句话：\\n\\n{sentences[0]}\\n\\n\")\n",
    "print(f\"Tokenize 后的第一句话: \\n\\n{tokenized_texts[0]}\\n\\n\")\n",
    "print(f\"Padding 后的第一句话： \\n\\n{input_ids[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] 写 在 年 末 冬 初 孩 子 流 感 的 第 五 天 ， 我 们 仍 然 没 有 忘 记 热 情 拥 抱 这 2020 年 的 第 一 天 。 带 着 一 丝 迷 信 ， 早 晨 给 孩 子 穿 上 红 色 的 羽 绒 服 羽 绒 裤 ， 祈 祷 新 的 一 年 ， 孩 子 们 身 体 康 健 。 仍 然 会 有 一 丝 焦 虑 ， 焦 虑 我 的 孩 子 为 什 么 会 过 早 的 懂 事 ， 从 两 岁 多 开 始 关 注 我 的 情 绪 ， 会 深 沉 地 说 ： 妈 妈 ， 你 终 于 笑 了 ！ 这 句 话 像 刀 子 一 样 扎 入 我? 展 开 全 文 c [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 转换回来\n",
    "raw_texts = [tokenizer.decode(input_ids[0])]\n",
    "print(raw_texts)\n",
    "print(len(raw_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT输入准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# 第一句话的 attention_masks\n",
    "print(np.array(attention_masks[0]))\n",
    "print(len(np.array(attention_masks[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99560\n",
      "['0', '-1', '1', '1', '1', '-1', '-1', '0', '-1', '1']\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 2, 2, 0, 0, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "clean_labels = []\n",
    "for label in labels:\n",
    "    clean_labels.append(int(label) + 1)  # 我们把标签变成非负数\n",
    "\n",
    "print(clean_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, clean_labels, \n",
    "                                                            random_state=2020, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2, 2, 2, 2, 2, 1, 2]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      标签总数： 99560\n",
      "训练集标签总数： 79648\n",
      "验证集标签总数： 19912\n"
     ]
    }
   ],
   "source": [
    "print(f\"      标签总数：\", len(labels))\n",
    "print(f\"训练集标签总数：\", len(train_labels))\n",
    "print(f\"验证集标签总数：\", len(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor化\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19912\n",
      "19912\n",
      "19912\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_inputs))\n",
    "print(len(validation_labels))\n",
    "print(len(validation_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形成训练数据集\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)  \n",
    "# 随机采样\n",
    "train_sampler = RandomSampler(train_data) \n",
    "# 读取数据\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# 形成验证数据集\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "# 随机采样\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "# 读取数据\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# 统计标签种类\n",
    "label_count = len(set(labels))\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取 BertForSequenceClassification 模型，\n",
    "# 是一个预训练的BERT模型，在最后面加了一个线形层用于分类。\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"../BERT-NER/bert-chinese/\", \n",
    "                                                      num_labels=label_count)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# 权重衰减\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score(y_true, y_pred, average='macro')  \n",
    "\n",
    "# 准确率计算函数\n",
    "def flat_f1(preds, labels):\n",
    "    \"\"\"f1计算，包括对logits的处理\"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(pred_flat, labels_flat, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存loss\n",
    "train_loss_set = []\n",
    "# epochs \n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前epoch： 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1992it [59:45,  1.80s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 epoch 的 Train loss: 0.5992388225583187\n",
      "当前epoch： 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1992it [59:51,  1.80s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 epoch 的 Train loss: 0.5052741847812651\n",
      "当前epoch： 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1992it [59:53,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 epoch 的 Train loss: 0.41739396379894045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT training loop\n",
    "for _ in range(epochs): \n",
    "    ## 训练\n",
    "    print(f\"当前epoch： {_}\")\n",
    "    # 开启训练模式\n",
    "    model.train()\n",
    "    tr_loss = 0  # train loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        # 把batch放入GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # 解包batch\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播loss计算\n",
    "        output = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels)  # 有labels的时候，且labels>1就直接返回Cross-Entropy\n",
    "        loss = output[0]\n",
    "        # print(loss)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        # 更新模型参数\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "    print(f\"当前 epoch 的 Train loss: {tr_loss/nb_tr_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证状态\n",
    "model.eval()\n",
    "\n",
    "# 建立变量\n",
    "eval_loss, eval_f1 = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "# Evaluate data for one epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model_save/0305_LocalCVf1_0.7184/vocab.txt',\n",
       " './model_save/0305_LocalCVf1_0.7184/special_tokens_map.json',\n",
       " './model_save/0305_LocalCVf1_0.7184/added_tokens.json')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "\n",
    "output_dir = \"./model_save/0305_LocalCVf1_0.7184/\"\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(model_to_save.state_dict(), os.path.join(output_dir, 'training_args_0304_1.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取模型\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "output_dir = \"./model_save\"\n",
    "model = BertForTokenClassification.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型本地验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [05:11<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.7184718949251692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 验证集的读取也要batch\n",
    "\n",
    "pred_valid_labels = []\n",
    "true_valid_labels = []\n",
    "\n",
    "for batch in tqdm(validation_dataloader):\n",
    "    # 元组打包放进GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # 解开元组\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # 预测\n",
    "    with torch.no_grad():\n",
    "        # segment embeddings，如果没有就是全0，表示单句\n",
    "        # position embeddings，[0,句子长度-1]\n",
    "        logits = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       token_type_ids=None,\n",
    "                       position_ids=None)  \n",
    "                       \n",
    "    # print(logits[0])\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[0].detach().cpu().numpy()  # 注意这里的logits是在softmax之前，所以和不为1\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # 由于F1的计算方式特殊，所以这里必须拿全部结果再计算F1\n",
    "    pred_valid_labels += list(pred_flat)\n",
    "    true_valid_labels += list(label_ids)\n",
    "    \n",
    "\n",
    "print(f\"Validation F1: {f1_score(np.array(pred_valid_labels), np.array(true_valid_labels), average='macro')}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0, 1, 1, 1, 0, 1, 2]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_valid_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 0, 1, 2]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_valid_labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./input/test_dataset/nCov_10k_test_utf8.csv', encoding='utf-8')\n",
    "\n",
    "test.rename(columns = {\"微博id\": \"Weibo_ID\",\n",
    "                        \"微博发布时间\": \"Publish_Time\", \n",
    "                        \"发布人账号\": \"Account_ID\",\n",
    "                        \"微博中文内容\": \"Chinese_Content\",\n",
    "                        \"微博图片\": \"Pictures\",\n",
    "                        \"微博视频\": \"Videos\",\n",
    "                        \"情感倾向\": \"Labels\"},  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weibo_ID</th>\n",
       "      <th>Publish_Time</th>\n",
       "      <th>Account_ID</th>\n",
       "      <th>Chinese_Content</th>\n",
       "      <th>Pictures</th>\n",
       "      <th>Videos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4456068992182160</td>\n",
       "      <td>01月01日 23:38</td>\n",
       "      <td>-精緻的豬豬女戰士-</td>\n",
       "      <td>#你好2020#新年第一天元气满满的早起出门买早饭结果高估了自己抗冻能力回家成功冻发烧（大概...</td>\n",
       "      <td>['https://ww2.sinaimg.cn/thumb150/745aa591ly1g...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4456424178427250</td>\n",
       "      <td>01月02日 23:09</td>\n",
       "      <td>liujunyi88</td>\n",
       "      <td>大宝又感冒鼻塞咳嗽了，还有发烧。队友加班几天不回。感觉自己的情绪在家已然是随时引爆的状态。情...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4456797466940200</td>\n",
       "      <td>01月03日 23:53</td>\n",
       "      <td>ablsa</td>\n",
       "      <td>还要去输两天液，这天也太容易感冒发烧了，一定要多喝热水啊?</td>\n",
       "      <td>['https://ww3.sinaimg.cn/orj360/006fTidCly1gaj...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Weibo_ID  Publish_Time  Account_ID  \\\n",
       "0  4456068992182160  01月01日 23:38  -精緻的豬豬女戰士-   \n",
       "1  4456424178427250  01月02日 23:09  liujunyi88   \n",
       "2  4456797466940200  01月03日 23:53       ablsa   \n",
       "\n",
       "                                     Chinese_Content  \\\n",
       "0  #你好2020#新年第一天元气满满的早起出门买早饭结果高估了自己抗冻能力回家成功冻发烧（大概...   \n",
       "1  大宝又感冒鼻塞咳嗽了，还有发烧。队友加班几天不回。感觉自己的情绪在家已然是随时引爆的状态。情...   \n",
       "2                      还要去输两天液，这天也太容易感冒发烧了，一定要多喝热水啊?   \n",
       "\n",
       "                                            Pictures Videos  \n",
       "0  ['https://ww2.sinaimg.cn/thumb150/745aa591ly1g...     []  \n",
       "1                                                 []     []  \n",
       "2  ['https://ww3.sinaimg.cn/orj360/006fTidCly1gaj...     []  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前几句\n",
    "test.iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [content for content in test['Chinese_Content'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9963\n",
      "['#你好2020#新年第一天元气满满的早起出门买早饭结果高估了自己抗冻能力回家成功冻发烧（大概是想告诉我2020要量力而行）然鹅这并不影响后续计划一出门立马生龙活虎新年和新??更配哦??看了误杀吃了大餐就让新的一年一直这样美滋滋下去吧???', '大宝又感冒鼻塞咳嗽了，还有发烧。队友加班几天不回。感觉自己的情绪在家已然是随时引爆的状态。情绪一上来，容易对孩子说出自己都想不到的话来……2020年，真的要学会控制情绪，管理好家人健康。这是今年最大的目标。?', '还要去输两天液，这天也太容易感冒发烧了，一定要多喝热水啊?']\n"
     ]
    }
   ],
   "source": [
    "print(len(test_sentences))\n",
    "print(test_sentences[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('../BERT-NER/bert-chinese/', do_lower_case=True)\n",
    "tokenized_test_texts = [tokenizer.encode(sent, add_special_tokens=True) for sent in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 前的第一句话：\n",
      "#你好2020#新年第一天元气满满的早起出门买早饭结果高估了自己抗冻能力回家成功冻发烧（大概是想告诉我2020要量力而行）然鹅这并不影响后续计划一出门立马生龙活虎新年和新??更配哦??看了误杀吃了大餐就让新的一年一直这样美滋滋下去吧???\n",
      "\n",
      "Tokenize 后的第一句话: \n",
      "[101, 108, 872, 1962, 8439, 108, 3173, 2399, 5018, 671, 1921, 1039, 3698, 4007, 4007, 4638, 3193, 6629, 1139, 7305, 743, 3193, 7649, 5310, 3362, 7770, 844, 749, 5632, 2346, 2834, 1108, 5543, 1213, 1726, 2157, 2768, 1216, 1108, 1355, 4173, 8020, 1920, 3519, 3221, 2682, 1440, 6401, 2769, 8439, 6206, 7030, 1213, 5445, 6121, 8021, 4197, 7900, 6821, 2400, 679, 2512, 1510, 1400, 5330, 6369, 1153, 671, 1139, 7305, 4989, 7716, 4495, 7987, 3833, 5988, 3173, 2399, 1469, 3173, 136, 136, 3291, 6981, 1521, 136, 136, 4692, 749, 6428, 3324, 1391, 749, 1920, 7623, 2218, 6375, 3173, 4638, 671, 2399, 671, 4684, 6821, 3416, 5401, 3996, 3996, 678, 1343, 1416, 136, 136, 136, 102]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenize 前的第一句话：\\n{test_sentences[0]}\\n\")\n",
    "print(f\"Tokenize 后的第一句话: \\n{tokenized_test_texts[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子最长长度\n",
    "MAX_LEN = 300\n",
    "\n",
    "# 输入padding\n",
    "# 此函数在keras里面\n",
    "input_test_ids = pad_sequences([txt for txt in tokenized_test_texts],\n",
    "                          maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", \n",
    "                          truncating=\"post\", \n",
    "                          padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 前的第一句话：\n",
      "\n",
      "#你好2020#新年第一天元气满满的早起出门买早饭结果高估了自己抗冻能力回家成功冻发烧（大概是想告诉我2020要量力而行）然鹅这并不影响后续计划一出门立马生龙活虎新年和新??更配哦??看了误杀吃了大餐就让新的一年一直这样美滋滋下去吧???\n",
      "\n",
      "\n",
      "Tokenize 后的第一句话: \n",
      "\n",
      "[101, 108, 872, 1962, 8439, 108, 3173, 2399, 5018, 671, 1921, 1039, 3698, 4007, 4007, 4638, 3193, 6629, 1139, 7305, 743, 3193, 7649, 5310, 3362, 7770, 844, 749, 5632, 2346, 2834, 1108, 5543, 1213, 1726, 2157, 2768, 1216, 1108, 1355, 4173, 8020, 1920, 3519, 3221, 2682, 1440, 6401, 2769, 8439, 6206, 7030, 1213, 5445, 6121, 8021, 4197, 7900, 6821, 2400, 679, 2512, 1510, 1400, 5330, 6369, 1153, 671, 1139, 7305, 4989, 7716, 4495, 7987, 3833, 5988, 3173, 2399, 1469, 3173, 136, 136, 3291, 6981, 1521, 136, 136, 4692, 749, 6428, 3324, 1391, 749, 1920, 7623, 2218, 6375, 3173, 4638, 671, 2399, 671, 4684, 6821, 3416, 5401, 3996, 3996, 678, 1343, 1416, 136, 136, 136, 102]\n",
      "\n",
      "\n",
      "Padding  后的第一句话： \n",
      "\n",
      "[ 101  108  872 1962 8439  108 3173 2399 5018  671 1921 1039 3698 4007\n",
      " 4007 4638 3193 6629 1139 7305  743 3193 7649 5310 3362 7770  844  749\n",
      " 5632 2346 2834 1108 5543 1213 1726 2157 2768 1216 1108 1355 4173 8020\n",
      " 1920 3519 3221 2682 1440 6401 2769 8439 6206 7030 1213 5445 6121 8021\n",
      " 4197 7900 6821 2400  679 2512 1510 1400 5330 6369 1153  671 1139 7305\n",
      " 4989 7716 4495 7987 3833 5988 3173 2399 1469 3173  136  136 3291 6981\n",
      " 1521  136  136 4692  749 6428 3324 1391  749 1920 7623 2218 6375 3173\n",
      " 4638  671 2399  671 4684 6821 3416 5401 3996 3996  678 1343 1416  136\n",
      "  136  136  102    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenize 前的第一句话：\\n\\n{test_sentences[0]}\\n\\n\")\n",
    "print(f\"Tokenize 后的第一句话: \\n\\n{tokenized_test_texts[0]}\\n\\n\")\n",
    "print(f\"Padding  后的第一句话： \\n\\n{input_test_ids[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建attention masks\n",
    "test_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_test_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    test_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor化\n",
    "test_inputs = torch.tensor(input_test_ids)\n",
    "\n",
    "# validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "test_masks = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9963\n",
      "9963\n"
     ]
    }
   ],
   "source": [
    "assert (len(test_inputs) == len(test_masks))\n",
    "print(len(test_inputs))\n",
    "print(len(test_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形成预测数据集\n",
    "test_data = TensorDataset(test_inputs, test_masks)\n",
    "# 随机采样\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "# 读取数据\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:35<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# 验证集的读取也要batch\n",
    "\n",
    "pred_test_labels = []\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    # 元组打包放进GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # 解开元组\n",
    "    b_input_ids, b_input_mask = batch\n",
    "    # 预测\n",
    "    with torch.no_grad():\n",
    "        # segment embeddings，如果没有就是全0，表示单句\n",
    "        # position embeddings，[0,句子长度-1]\n",
    "        logits = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       token_type_ids=None,\n",
    "                       position_ids=None)  \n",
    "                       \n",
    "    # print(logits[0])\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[0].detach().cpu().numpy()  # 注意这里的logits是在softmax之前，所以和不为1\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    # label_ids = b_labels.to('cpu').numpy()  # 真实ID\n",
    "    \n",
    "    # 由于F1的计算方式特殊，所以这里必须拿全部结果再计算F1\n",
    "    pred_test_labels += list(pred_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 0, 2, 0, 1, 0, 1, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0]\n",
      "9963\n"
     ]
    }
   ],
   "source": [
    "print(pred_test_labels[0:20])\n",
    "print(len(pred_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred'] = pred_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.rename(columns = {\"Weibo_ID\": \"id\"},  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Publish_Time</th>\n",
       "      <th>Account_ID</th>\n",
       "      <th>Chinese_Content</th>\n",
       "      <th>Pictures</th>\n",
       "      <th>Videos</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4456068992182160</td>\n",
       "      <td>01月01日 23:38</td>\n",
       "      <td>-精緻的豬豬女戰士-</td>\n",
       "      <td>#你好2020#新年第一天元气满满的早起出门买早饭结果高估了自己抗冻能力回家成功冻发烧（大概...</td>\n",
       "      <td>['https://ww2.sinaimg.cn/thumb150/745aa591ly1g...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4456424178427250</td>\n",
       "      <td>01月02日 23:09</td>\n",
       "      <td>liujunyi88</td>\n",
       "      <td>大宝又感冒鼻塞咳嗽了，还有发烧。队友加班几天不回。感觉自己的情绪在家已然是随时引爆的状态。情...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4456797466940200</td>\n",
       "      <td>01月03日 23:53</td>\n",
       "      <td>ablsa</td>\n",
       "      <td>还要去输两天液，这天也太容易感冒发烧了，一定要多喝热水啊?</td>\n",
       "      <td>['https://ww3.sinaimg.cn/orj360/006fTidCly1gaj...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4456791021108920</td>\n",
       "      <td>01月03日 23:27</td>\n",
       "      <td>喵吃鱼干Lynn</td>\n",
       "      <td>我太难了别人怎么发烧都没事就我一检查甲型流感?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4457086404997440</td>\n",
       "      <td>01月04日 19:01</td>\n",
       "      <td>我的发小今年必脱单</td>\n",
       "      <td>果然是要病一场的喽回来第三天开始感冒今儿还发烧了喉咙眼睛都难受的一匹怎么样能不经意让我的毕设...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  Publish_Time  Account_ID  \\\n",
       "0  4456068992182160  01月01日 23:38  -精緻的豬豬女戰士-   \n",
       "1  4456424178427250  01月02日 23:09  liujunyi88   \n",
       "2  4456797466940200  01月03日 23:53       ablsa   \n",
       "3  4456791021108920  01月03日 23:27    喵吃鱼干Lynn   \n",
       "4  4457086404997440  01月04日 19:01   我的发小今年必脱单   \n",
       "\n",
       "                                     Chinese_Content  \\\n",
       "0  #你好2020#新年第一天元气满满的早起出门买早饭结果高估了自己抗冻能力回家成功冻发烧（大概...   \n",
       "1  大宝又感冒鼻塞咳嗽了，还有发烧。队友加班几天不回。感觉自己的情绪在家已然是随时引爆的状态。情...   \n",
       "2                      还要去输两天液，这天也太容易感冒发烧了，一定要多喝热水啊?   \n",
       "3                            我太难了别人怎么发烧都没事就我一检查甲型流感?   \n",
       "4  果然是要病一场的喽回来第三天开始感冒今儿还发烧了喉咙眼睛都难受的一匹怎么样能不经意让我的毕设...   \n",
       "\n",
       "                                            Pictures Videos  pred  \n",
       "0  ['https://ww2.sinaimg.cn/thumb150/745aa591ly1g...     []     1  \n",
       "1                                                 []     []     1  \n",
       "2  ['https://ww3.sinaimg.cn/orj360/006fTidCly1gaj...     []     1  \n",
       "3                                                 []     []     0  \n",
       "4                                                 []     []     0  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于存在空值，我们一开始做了 `dropna` ，所以要填写回去再`fillna(0)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv('./input/submit_example.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4456068992182160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4456424178427250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4456797466940200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4456791021108920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4457086404997440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>4464179518243680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>4464274073923100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>4464289160945130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>4465347950314820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>4465492650005690</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  y\n",
       "0     4456068992182160  0\n",
       "1     4456424178427250  0\n",
       "2     4456797466940200  0\n",
       "3     4456791021108920  0\n",
       "4     4457086404997440  0\n",
       "...                ... ..\n",
       "9995  4464179518243680  0\n",
       "9996  4464274073923100  0\n",
       "9997  4464289160945130  0\n",
       "9998  4465347950314820  0\n",
       "9999  4465492650005690  0\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub2 = sample_sub.merge(test, on='id', how=\"outer\").drop(['Publish_Time', 'Account_ID', 'Chinese_Content', 'Pictures', 'Videos'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4456068992182160</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4456424178427250</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4456797466940200</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4456791021108920</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4457086404997440</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  y  pred\n",
       "0  4456068992182160  0   1.0\n",
       "1  4456424178427250  0   1.0\n",
       "2  4456797466940200  0   1.0\n",
       "3  4456791021108920  0   0.0\n",
       "4  4457086404997440  0   0.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub2 = sample_sub2.drop(['y'], axis=1)\n",
    "sample_sub2.rename(columns = {\"pred\": \"y\"},  inplace=True)\n",
    "sample_sub2 = sample_sub2.fillna(0)  # 一开始是把空的排除了的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 别忘了给标签-1\n",
    "\n",
    "sample_sub2['y'] = sample_sub2['y'].apply(lambda x: int(x - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4456068992182160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4456424178427250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4456797466940200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4456791021108920</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4457086404997440</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  y\n",
       "0  4456068992182160  0\n",
       "1  4456424178427250  0\n",
       "2  4456797466940200  0\n",
       "3  4456791021108920 -1\n",
       "4  4457086404997440 -1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    5625\n",
       " 1    2940\n",
       "-1    1435\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub2['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub2.to_csv(\"NimenDavid_submission_0305_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
